{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a CNN for Image Classification (vs the fully connected NN)\n",
    "### Working with the MNIST dataset ~ The *Hello World* of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is identical to [01_train_mnist_classifier_simple_NN.ipynb](01_train_mnist_classifier_simple_NN.ipynb), except for the model's architecture.\n",
    "\n",
    "Due to the similarities of the two notebooks, here I only provide the absolute necessary steps through which one needs to go through. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision as tv\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets \n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get cpu, gpu or mps device for training\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create your directory structure for your datasets and models\n",
    "\n",
    "data_dir = pathlib.Path(\"datasets\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "models_dir = pathlib.Path(\"models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "model_name = \"mnist_cnn\" # change when working with other datasets\n",
    "\n",
    "mnist_dir = models_dir / model_name\n",
    "mnist_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Processing ~ Image Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = v2.Compose([\n",
    "    v2.ToImage(),                         # an updated version of the older conversion .ToTensor()\n",
    "    v2.ToDtype(torch.float32, scale=True) # scales the pixel values from [0, 255] to [0.0, 1.0]\n",
    "])\n",
    "\n",
    "train_data = datasets.MNIST(\n",
    "    root=data_dir,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms,\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=data_dir,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observing our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.data.shape, train_data.targets.shape)\n",
    "print(\"-----\")\n",
    "print(test_data.data.shape, test_data.targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join(train_data.classes)) # join an array into a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_num = 7 # change this number to have a glimpse into another item of the trainset\n",
    "\n",
    "# plotting for one image\n",
    "plt.figure()\n",
    "plt.title(f\"Label: {train_data.targets[img_num]}\")\n",
    "plt.imshow(train_data.data[img_num], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting for multiple images, randomly selected\n",
    "figure = plt.figure(figsize=(10, 8))\n",
    "cols, rows = 5, 5\n",
    "for i in range(1, cols * rows + 1):\n",
    "    # generate a random index between 0 and len(mnist_trainset)-1, inclusive\n",
    "    sample_idx = torch.randint(len(train_data), size=(1,)).item()\n",
    "    # retrieve the image and the respective label for that index\n",
    "    img, label = train_data[sample_idx]\n",
    "    # create the grid of subplots within the bigger plot\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(label)\n",
    "    plt.axis(\"off\")\n",
    "    # squeeze() removes all dimensions with size 1\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Training Workflow\n",
    "\n",
    "Heavily based on Jérémie Wenger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "- **Defining our NN, our Model**\n",
    "- **Defining our Optimizer and Loss Function** \n",
    "- **Implementing the Training Loop**   \n",
    "- **Testing our Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our Convolutional Neural Network\n",
    "\n",
    "For estimating the input and output dimensions of our data in the NN below, one needs to remember that MNIST contains images of single digits, so 10 classes, from 0 to 9. All images are grayscale (1 colour channel) with dimensions 28 * 28 pixels. In the case of a CNN network, the input is the number of colour channels, i.e. 1 for the grayscale MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNetwork, self).__init__()\n",
    "        # Input shape: [batch, 1, 28, 28]\n",
    "        # Breaking down the first conv layer: \n",
    "        #   > 1 input channel for grayscale images\n",
    "        #   > 32 different filters to output\n",
    "        #   > 3x3 kernel size\n",
    "        #   > 1 padding\n",
    "        # output shape: [batch, 32, 28, 28]\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        # 2x2 maxpooling, output shape: [batch, 32, 14, 14]\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # and so on and so forth ...\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1) \n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ConvNetwork(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ConvNetwork, self).__init__()\n",
    "#         # Convolutional feature extraction layers\n",
    "#         # Input: [batch, 1, 28, 28]\n",
    "#         # After conv1: [batch, 32, 28, 28]\n",
    "#         # After pool1: [batch, 32, 14, 14]\n",
    "#         # After conv2: [batch, 64, 14, 14]\n",
    "#         # After pool2: [batch, 64, 7, 7]\n",
    "#         self.features = nn.Sequential(\n",
    "#             nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2, 2),\n",
    "#             nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2, 2)\n",
    "#         )\n",
    "#         # Fully connected classification layers\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Flatten(),\n",
    "#             nn.Linear(64 * 7 * 7, 64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(64, 10)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.features(x)\n",
    "#         x = self.classifier(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Layers and their initial weights/bias shapes:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\" - {name} | Shape: {param.shape} | Sample values: {param.data.flatten()[:5]}...\")\n",
    "\n",
    "print()\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing our Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the training loop\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset) # total number of samples in the dataset to track progress\n",
    "    losses, accs = [], [] # to store loss and accuracy history\n",
    "\n",
    "    model.train() # set the model to training mode\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        \n",
    "        # 0: data & target to specified device\n",
    "        X, y = data.to(device), target.to(device)\n",
    "        \n",
    "        # 1: prediction - getting logits for each class\n",
    "        pred = model(X)\n",
    "\n",
    "        # 2: loss\n",
    "        loss = loss_fn(pred, y)\n",
    "    \n",
    "        # 3: backpropagation - computes gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # 4: update parameters based on computed gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # 5: 'zero grad' (otherwise the gradients remain there)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Logging & saving history\n",
    "\n",
    "        # save losses\n",
    "        losses.append(loss.item())\n",
    "        # save our accuracy\n",
    "        accs.append((pred.argmax(1) == y).type(torch.float).mean().item())\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            loss, current = loss.item(), (batch_idx + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    print()\n",
    "    return losses, accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the test loop\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    \n",
    "    losses, accs = [], []\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    model.eval() # set the model to test mode\n",
    "\n",
    "    # no gradients since we are not training!\n",
    "    with torch.no_grad():\n",
    "        for (data, target) in dataloader:\n",
    "\n",
    "            # 0: data & target to specified device\n",
    "            X, y = data.to(device), target.to(device)\n",
    "\n",
    "            # 1: prediction - getting logits for each class\n",
    "            pred = model(X)\n",
    "            \n",
    "            # 2: loss and accuracy\n",
    "            loss = loss_fn(pred, y)\n",
    "            t_l = loss.item()\n",
    "            test_loss += t_l\n",
    "\n",
    "            # accumulate our accuracy\n",
    "            a = (pred.argmax(1) == y).type(torch.float)\n",
    "            correct += a.sum().item()\n",
    "\n",
    "            # save loss and acc\n",
    "            losses.append(t_l)\n",
    "            accs.append(a.mean().item())\n",
    "    \n",
    "    # average loss & results\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    \n",
    "    print(\"Test Error:\")\n",
    "    print(f\"Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")\n",
    "    print()\n",
    "    \n",
    "    return losses, accs, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "train_losses, train_accs, test_losses, test_accs = [], [], [], []\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\")\n",
    "    print(\"-------------------------------\")\n",
    "    train_l, train_a = train(train_dataloader, model, loss_fn, optimizer) # per batches losses and accuracies\n",
    "    test_l, test_a, _ = test(test_dataloader, model, loss_fn) # per batches losses and accuracies\n",
    "    # save history\n",
    "    train_losses.extend(train_l)\n",
    "    train_accs.extend(train_a)\n",
    "    test_losses.extend(test_l)\n",
    "    test_accs.extend(test_a)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing ~ Evaluating the Performance of our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Train loss\")\n",
    "plt.plot(train_losses,label=\"train\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"cumulative loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = test(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using our Model on an Input Image\n",
    "\n",
    "\n",
    "See [Transforming and Augmenting Images](https://pytorch.org/vision/stable/transforms.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('images/3.png') # try also images/4.png\n",
    "\n",
    "transforms = v2.Compose([  \n",
    "    v2.Grayscale(num_output_channels=1),\n",
    "    v2.Resize(size=(28,28), antialias=True),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True), # from [0,255] to [0,1]\n",
    "])\n",
    "\n",
    "input = transforms(img).unsqueeze(0)  # ADD BATCH DIMENSION [1, 1, 28, 28]\n",
    "input = input.to(device)\n",
    "\n",
    "print(f\"Input shape: {input.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = nn.Softmax(dim=-1)(model(input)).cpu().numpy()\n",
    "print(f\"Our predictions (shape: {predictions.shape})\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# note that predictions is still *batched* (shape: (1,10)), we need to fetch the first array\n",
    "predicted = np.argmax(predictions[0]) # argmax: the *index* of the highest prediction\n",
    "\n",
    "plt.figure()\n",
    "plt.title(f'Predicted number: {train_data.classes[predicted]}') # use the predicted category in the title\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot our predictions for all classes using a [bar chart](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.bar.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.title(\"Predictions\")\n",
    "xs = train_data.classes     # 0 to 9 for Xs, our ys are our predictions\n",
    "plt.bar(xs, predictions[0]) # a bar chart\n",
    "plt.xticks(xs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.save(torch.jit.script(model), mnist_dir / f\"my_{model_name}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving & Loading Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# save (reload using torch.jit.load)\n",
    "torch.jit.save(torch.jit.script(model), mnist_dir / f\"my_{model_name}.pt\")\n",
    "\n",
    "# save (reload using model.load_state_dict, requires the model class, ie you need to redefine your model architecture)\n",
    "torch.save(model.state_dict(), mnist_dir / f\"{model_name}.pt\")\n",
    "print(f\"Saved PyTorch Model State to {mnist_dir / model_name}.pt\")\n",
    "\n",
    "# instantiate then load (you need to have defined NeuralNetwork)!\n",
    "model_reloaded = NeuralNetwork().to(device)\n",
    "model_reloaded.load_state_dict(torch.load(mnist_dir / f\"{model_name}.pt\", weights_only=True))\n",
    "```\n",
    "The `jit` only method is ideal for using model (inference), **however**, if you want to finetune your model after reloading it, prefer the full method above (class definition + loading weights)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "dmlcp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
