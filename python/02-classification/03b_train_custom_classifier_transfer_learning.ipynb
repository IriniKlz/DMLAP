{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d1f6d3e",
   "metadata": {},
   "source": [
    "## Training a CNN for Image Classification on your Custom Dataset: \n",
    "## Training from Scratch VS Tranfer Learning\n",
    "\n",
    "This notebook combines two approaches: **From Scratch** where we build and train a custom CNN from the ground up (identical to [notebook 03](python/02-classification/03_train_custom_classifier_CNN.ipynb)), and **Transfer Learning with Pre-Trained Models** where we leverage pre-trained models (ResNet18) for faster and potentially better convergence.\n",
    "\n",
    "Reference: [PyTorch Transfer Learning Tutorial](https://docs.pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)\n",
    "\n",
    "Before you proceed with this notebook, you need to have a custom dataset to train your model on. You may use one of the methods suggested in [04_collect_data.ipynb](04_collect_data.ipynb) and make sure your custom dataset in under your `./datasets` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d08d56",
   "metadata": {},
   "source": [
    "## Part 1: Training from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185e7b4c",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b102132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from PIL import Image\n",
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision as tv\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.datasets as datasets \n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6a9aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cpu, gpu or mps device for training\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35301976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your directory structure for your datasets and models\n",
    "\n",
    "data_dir = pathlib.Path(\"datasets/gallery_dl_dataset\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "models_dir = pathlib.Path(\"models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "model_name = \"image_classifier\" # change when working with other datasets\n",
    "\n",
    "model_dir = models_dir / model_name\n",
    "model_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c104d7d6",
   "metadata": {},
   "source": [
    "### Data Processing ~ Image Transformations\n",
    "\n",
    "Could you augment your training data by adding more transformations to them?\n",
    "\n",
    "You could randomly change their brightness, contrast, saturation, and hue.\n",
    "\n",
    "You could flip them horizontally or vertically with a 0.5 probability.\n",
    "\n",
    "You could randomly rotate them.\n",
    "\n",
    "Look in [here](https://pytorch.org/vision/stable/transforms.html) and [here](https://pytorch.org/vision/stable/auto_examples/transforms/plot_transforms_illustrations.html#sphx-glr-auto-examples-transforms-plot-transforms-illustrations-py) for references and examples. \n",
    "\n",
    "Do you need to also add the above transformations to your validation set? Or are the existing ones enough? You need to consider what the purpose of each dataset is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844b1b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3 # your number of classes\n",
    "\n",
    "train_transform = v2.Compose([\n",
    "        # v2.Resize(size=(64, 64), antialias=True),\n",
    "        v2.RandomResizedCrop(size=(64, 64), antialias=True),\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True), \n",
    "    ])\n",
    "\n",
    "val_transform = v2.Compose([\n",
    "        v2.Resize(size=(64, 64), antialias=True),\n",
    "        v2.CenterCrop(size=(64, 64)),\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True), \n",
    "    ])\n",
    "\n",
    "# create train and validation datasets with seperate transforms\n",
    "train_dataset = datasets.ImageFolder(data_dir, transform=train_transform)\n",
    "val_dataset = datasets.ImageFolder(data_dir, transform=val_transform)\n",
    "test_dataset = datasets.ImageFolder(data_dir, transform=val_transform)\n",
    "\n",
    "print(\"\\n\".join(train_dataset.classes)) # should show the folder names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955222b8",
   "metadata": {},
   "source": [
    "Here we create our train, validation and test datasets by splitting the full input dataset into three subsets. A 70-20-10 split is quite common.\n",
    "\n",
    "By setting a `random_state`, we are performing the split randomly but in a deterministic way, i.e. we will always get the same random train_test_split as long as we use the same random_state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d379a48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get length of the full dataset before split, and save it in idx\n",
    "num_train = len(train_dataset)\n",
    "\n",
    "# define the percentage that will be used for validation\n",
    "val_size = 0.2\n",
    "test_size = 0.1  \n",
    "\n",
    "# create an array of idx numbers for each element of the full dataset\n",
    "idx = list(range(num_train))\n",
    "print(num_train, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706f816d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform train / val split for data points\n",
    "train_indices, val_indices = train_test_split(idx, test_size=val_size, random_state=42)\n",
    "train_indices, test_indices = train_test_split(train_indices, test_size=test_size/(1 - val_size), random_state=42)  \n",
    "\n",
    "# override datasets to only be samples for each split\n",
    "train_dataset = Subset(train_dataset, train_indices)\n",
    "val_dataset = Subset(val_dataset, val_indices)\n",
    "test_dataset = Subset(test_dataset, test_indices)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a55d1ed",
   "metadata": {},
   "source": [
    "### Observing our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f01c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset sizes and sample shape\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "img_num = 92 # change this number to view a different sample\n",
    "\n",
    "# Get a sample to check shape\n",
    "sample_img, sample_label = train_dataset[img_num]\n",
    "print(f\"\\nSample image shape: {sample_img.shape}\")\n",
    "print(f\"Sample label: {sample_label}\")\n",
    "print(f\"Classes: {train_dataset.dataset.classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eef255a",
   "metadata": {},
   "source": [
    "### Visualising Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a1c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample image and its label\n",
    "sample_img, sample_label = train_dataset[img_num]\n",
    "label_name = train_dataset.dataset.classes[sample_label]\n",
    "\n",
    "# Plot it\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.title(f\"Label: {label_name} (index: {sample_label})\")\n",
    "plt.imshow(sample_img.permute(1, 2, 0))  # Convert from (C, H, W) to (H, W, C) for display\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136fb2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting for multiple images, randomly selected\n",
    "figure = plt.figure(figsize=(12, 10))\n",
    "cols, rows = 5, 5\n",
    "for i in range(1, cols * rows + 1):\n",
    "    # generate a random index\n",
    "    sample_idx = torch.randint(len(train_dataset), size=(1,)).item()\n",
    "    # retrieve the image and the respective label for that index\n",
    "    img, label = train_dataset[sample_idx]\n",
    "    label_name = train_dataset.dataset.classes[label]\n",
    "    \n",
    "    # create the grid of subplots\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(label_name, fontsize=8)\n",
    "    plt.axis(\"off\")\n",
    "    # Convert from (C, H, W) to (H, W, C) for display\n",
    "    plt.imshow(img.permute(1, 2, 0))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d6db68",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17027da",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 6\n",
    "\n",
    "# create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for X, y in val_loader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e4a59e",
   "metadata": {},
   "source": [
    "### Defining our Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca357c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNetwork, self).__init__()\n",
    "        # Input shape: [batch, 3, 64, ]\n",
    "        # Breaking down the first conv layer: \n",
    "        #   > 1 input channel for grayscale images\n",
    "        #   > 32 different filters to output\n",
    "        #   > 3x3 kernel size\n",
    "        #   > 1 padding\n",
    "        # output shape: [batch, 64, 64, 64]\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        # 2x2 maxpooling, output shape: [batch, 64, 32, 32]\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # and so on and so forth ...\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 256)\n",
    "        self.fc2 = nn.Linear(256, 3) # change the output size to match your number of classes\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1) \n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0029fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a00b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Layers and their initial weights/bias shapes:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\" - {name} | Shape: {param.shape} | Sample values: {param.data.flatten()[:5]}...\")\n",
    "\n",
    "print()\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d13b4da",
   "metadata": {},
   "source": [
    "### Optimizer and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a0f2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3876f48",
   "metadata": {},
   "source": [
    "### Implementing our Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94595598",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs): \n",
    "    train_loss = 0.0\n",
    "    \n",
    "    # training loop\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # get data\n",
    "        inputs = data.to(device)\n",
    "        labels = target.to(device)\n",
    "        \n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        predictions = model(inputs)\n",
    "        # compute the loss\n",
    "        loss = loss_fn(predictions, labels)\n",
    "        # backpropagate\n",
    "        loss.backward()\n",
    "        # update the parameters, i.e. weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # save statistics to plot later\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # validation loop\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        for batch_idx, (data, target) in enumerate(val_loader):\n",
    "            # get data\n",
    "            inputs = data.to(device)\n",
    "            labels = target.to(device)\n",
    "            # forward pass, no backpropagation and optimisation\n",
    "            predictions = model(inputs)\n",
    "            # compute the loss\n",
    "            loss = loss_fn(predictions, labels)\n",
    "            # save statistics to plot later\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    # normalise cumulative losses to dataset size\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    # added cumulative losses to lists to plot later\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}, train loss: {train_loss:.3f}, val loss: {val_loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7decac65",
   "metadata": {},
   "source": [
    "### Testing ~ Evaluating the Performance of our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d0d5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Train vs validation loss - From Scratch\")\n",
    "plt.plot(train_losses,label=\"train\")\n",
    "plt.plot(val_losses,label=\"val\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"cumulative loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce837942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn, device=device):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    # average loss across batches and accuracy across samples\n",
    "    test_loss = test_loss / num_batches\n",
    "    accuracy = correct / size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*accuracy):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss, accuracy\n",
    "\n",
    "# Run test on the test loader\n",
    "test_loss, test_acc = test(test_loader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac223c5",
   "metadata": {},
   "source": [
    "### Using our Model on an Input Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47872ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('images/colorful-carpet-sample.png') # try also images/4.png\n",
    "\n",
    "transforms = v2.Compose([  \n",
    "    # v2.Grayscale(num_output_channels=1),\n",
    "    v2.Resize(size=(64,64), antialias=True),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "])\n",
    "\n",
    "input = transforms(img).unsqueeze(0)  # ADD BATCH DIMENSION [1, 1, 28, 28]\n",
    "input = input.to(device)\n",
    "\n",
    "print(f\"Input shape: {input.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0263fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = nn.Softmax(dim=-1)(model(input)).cpu().numpy()\n",
    "print(f\"Our predictions (shape: {predictions.shape})\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566a7cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = np.argmax(predictions[0]) # argmax: the *index* of the highest prediction\n",
    "\n",
    "plt.figure()\n",
    "plt.title(f'Predicted number: {train_dataset.dataset.classes[predicted]}') # use the predicted category in the title\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f45708",
   "metadata": {},
   "source": [
    "We can plot our predictions for all classes using a [bar chart](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.bar.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c5a341",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.title(\"Predictions - From Scratch\")\n",
    "xs = train_dataset.dataset.classes     # 0 to 9 for Xs, our ys are our predictions\n",
    "plt.bar(xs, predictions[0]) # a bar chart\n",
    "plt.xticks(xs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b07acc2",
   "metadata": {},
   "source": [
    "### Save our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2aa95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.save(torch.jit.script(model), model_dir / f\"my_{model_name}_from_scratch.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a9db48",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9312106",
   "metadata": {},
   "source": [
    "## Part 2: Transfer Learning & Fine-tuning with Pre-trained Models\n",
    "\n",
    "In this section, we leverage transfer learning with pre-trained models from ImageNet. This approach is typically faster to train and can achieve better accuracy with limited data.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Fine-tuning**: Train all layers of a pre-trained model on your custom dataset\n",
    "- **Feature Extraction**: Freeze pre-trained layers and only train the final classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be57ced3",
   "metadata": {},
   "source": [
    "### ImageNet Normalization for Pre-trained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2edda3a",
   "metadata": {},
   "source": [
    "The pre-trained ResNet18 was trained on ImageNet with images that were normalized using those specific mean and std values [0.485, 0.456, 0.406] and [0.229, 0.224, 0.225]. Without the normalization step, our model receives input that doesn't match the distribution it learned from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074bcc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For transfer learning, we use ImageNet statistics for normalization\n",
    "# These are the mean and std values that ResNet and other pre-trained models were trained on\n",
    "\n",
    "transfer_train_transform = v2.Compose([\n",
    "    v2.RandomResizedCrop(224),\n",
    "    v2.RandomHorizontalFlip(),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                  std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "transfer_val_transform = v2.Compose([\n",
    "    v2.Resize(256),\n",
    "    v2.CenterCrop(224),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                  std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Create transfer learning datasets\n",
    "transfer_train_dataset = datasets.ImageFolder(data_dir, transform=transfer_train_transform)\n",
    "transfer_val_dataset = datasets.ImageFolder(data_dir, transform=transfer_val_transform)\n",
    "transfer_test_dataset = datasets.ImageFolder(data_dir, transform=transfer_val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e3119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for transfer learning (same indices as before for consistency)\n",
    "num_train_tl = len(transfer_train_dataset)\n",
    "idx_tl = list(range(num_train_tl))\n",
    "\n",
    "train_indices_tl, val_indices_tl = train_test_split(idx_tl, test_size=val_size, random_state=42)\n",
    "train_indices_tl, test_indices_tl = train_test_split(train_indices_tl, test_size=test_size/(1 - val_size), random_state=42)\n",
    "\n",
    "transfer_train_dataset = Subset(transfer_train_dataset, train_indices_tl)\n",
    "transfer_val_dataset = Subset(transfer_val_dataset, val_indices_tl)\n",
    "transfer_test_dataset = Subset(transfer_test_dataset, test_indices_tl)\n",
    "\n",
    "# Create dataloaders for transfer learning\n",
    "transfer_batch_size = 6  # 32 Can use larger batch size for feature extraction / pre-trained models \n",
    "\n",
    "transfer_train_loader = DataLoader(transfer_train_dataset, batch_size=transfer_batch_size, shuffle=True)\n",
    "transfer_val_loader = DataLoader(transfer_val_dataset, batch_size=transfer_batch_size, shuffle=False)\n",
    "transfer_test_loader = DataLoader(transfer_test_dataset, batch_size=transfer_batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Transfer Learning DataLoaders created\")\n",
    "print(f\"  Training samples: {len(transfer_train_dataset)}\")\n",
    "print(f\"  Validation samples: {len(transfer_val_dataset)}\")\n",
    "print(f\"  Test samples: {len(transfer_test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833dd60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample image and its label\n",
    "sample_img, sample_label = transfer_train_dataset[img_num]\n",
    "label_name = transfer_train_dataset.dataset.classes[sample_label]\n",
    "\n",
    "# Plot it\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.title(f\"Label: {label_name} (index: {sample_label})\")\n",
    "plt.imshow(sample_img.permute(1, 2, 0))  # Convert from (C, H, W) to (H, W, C) for display\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8e8fdf",
   "metadata": {},
   "source": [
    "### Approach 1: Fine-tuning the Pre-trained Model\n",
    "\n",
    "In [this approach](https://docs.pytorch.org/tutorials/beginner/transfer_learning_tutorial.html#finetuning-the-convnet), we:\n",
    "1. Load a pre-trained ResNet18 model\n",
    "2. Replace the final fully connected layer to match our number of classes\n",
    "3. Train all layers (updating all weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983cd9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained ResNet18 model\n",
    "model_ft = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Get the number of input features for the final layer\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "\n",
    "# Replace the final fully connected layer\n",
    "model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "# Move model to device\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "print(\"Pre-trained ResNet18 model loaded and final layer replaced\")\n",
    "print(f\"Final layer: {model_ft.fc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba329cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Layers and their initial weights/bias shapes:\")\n",
    "for name, param in model_ft.named_parameters():\n",
    "    print(f\" - {name} | Shape: {param.shape} | Sample values: {param.data.flatten()[:5]}...\")\n",
    "\n",
    "print()\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model_ft.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4690b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function for transfer learning\n",
    "def train_transfer_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, model_name=\"model\"):\n",
    "    \"\"\"\n",
    "    Train a model with validation loop and learning rate scheduling.\n",
    "    Saves the best model based on validation accuracy.\n",
    "    \"\"\"\n",
    "    since = time.time()\n",
    "    best_acc = 0.0\n",
    "    best_model_wts = deepcopy(model.state_dict())\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "                dataloader = train_loader\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "                dataloader = val_loader\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            total_samples = 0\n",
    "            \n",
    "            # Iterate over data\n",
    "            for inputs, labels in dataloader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    # Backward pass + optimize only in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                total_samples += inputs.size(0)\n",
    "            \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "            \n",
    "            epoch_loss = running_loss / total_samples\n",
    "            epoch_acc = running_corrects.float() / total_samples\n",
    "            \n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "            \n",
    "            # Deep copy the model if it has the best validation accuracy\n",
    "            if phase == 'val':\n",
    "                val_losses.append(epoch_loss)\n",
    "                epoch_acc_value = epoch_acc.item()\n",
    "                val_accs.append(epoch_acc_value)\n",
    "                if epoch_acc_value > best_acc:\n",
    "                    best_acc = epoch_acc_value\n",
    "                    best_model_wts = deepcopy(model.state_dict())\n",
    "            else:\n",
    "                train_losses.append(epoch_loss)\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:.4f}')\n",
    "    \n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, train_losses, val_losses, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c34a547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for fine-tuning\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = torch.optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "# Train and evaluate\n",
    "num_epochs_ft = 25\n",
    "\n",
    "print(\"Starting Fine-tuning...\")\n",
    "model_ft, train_losses_ft, val_losses_ft, val_accs_ft = train_transfer_model(\n",
    "    model_ft, transfer_train_loader, transfer_val_loader,\n",
    "    criterion, optimizer_ft, exp_lr_scheduler,\n",
    "    num_epochs_ft, \"fine_tuned\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79cca9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot fine-tuning results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Fine-tuning - Loss\")\n",
    "plt.plot(train_losses_ft, label=\"train\")\n",
    "plt.plot(val_losses_ft, label=\"val\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Fine-tuning - Validation Accuracy\")\n",
    "plt.plot(val_accs_ft, label=\"val accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983c80c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fine-tuned model\n",
    "def test_transfer(dataloader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss = test_loss / num_batches\n",
    "    accuracy = correct / size\n",
    "    print(f\"Test Error (Fine-tuned): \\n Accuracy: {(100*accuracy):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss, accuracy\n",
    "\n",
    "test_loss_ft, test_acc_ft = test_transfer(transfer_test_loader, model_ft, criterion, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de523a7f",
   "metadata": {},
   "source": [
    "### Approach 2: ConvNet as Fixed Feature Extractor\n",
    "\n",
    "In [this approach](https://docs.pytorch.org/tutorials/beginner/transfer_learning_tutorial.html#convnet-as-fixed-feature-extractor), we:\n",
    "1. Load a pre-trained ResNet18 model\n",
    "2. Freeze all layers except the final fully connected layer\n",
    "3. Train only the final classifier layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7deddc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load another copy of pre-trained ResNet18\n",
    "model_conv = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Freeze all the network parameters\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the final fully connected layer\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "# Move model to device\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "print(\"Pre-trained ResNet18 loaded as feature extractor\")\n",
    "print(\"All parameters frozen except final layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285f5ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for feature extraction\n",
    "criterion_conv = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only the final layer parameters are being optimized\n",
    "optimizer_conv = torch.optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler_conv = torch.optim.lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n",
    "\n",
    "# Train and evaluate\n",
    "num_epochs_conv = 25\n",
    "\n",
    "print(\"Starting Feature Extraction training...\")\n",
    "model_conv, train_losses_conv, val_losses_conv, val_accs_conv = train_transfer_model(\n",
    "    model_conv, transfer_train_loader, transfer_val_loader,\n",
    "    criterion_conv, optimizer_conv, exp_lr_scheduler_conv,\n",
    "    num_epochs_conv, \"feature_extractor\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1de3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature extraction results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Feature Extraction - Loss\")\n",
    "plt.plot(train_losses_conv, label=\"train\")\n",
    "plt.plot(val_losses_conv, label=\"val\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Feature Extraction - Validation Accuracy\")\n",
    "plt.plot(val_accs_conv, label=\"val accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18405488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the feature extractor model\n",
    "test_loss_conv, test_acc_conv = test_transfer(transfer_test_loader, model_conv, criterion_conv, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5805fbbf",
   "metadata": {},
   "source": [
    "### Comparison: From Scratch vs Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75475a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison plot\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.title(\"Test Accuracy Comparison\")\n",
    "approaches = ['From Scratch\\n(Custom CNN)', 'Fine-tuning\\n(ResNet18)', 'Feature Extraction\\n(ResNet18)']\n",
    "accuracies = [test_acc, test_acc_ft, test_acc_conv]\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "bars = plt.bar(approaches, accuracies, color=colors)\n",
    "plt.ylabel(\"Test Accuracy\")\n",
    "plt.ylim([0, 1])\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.2, \n",
    "             f'{acc:.1%}', ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62084c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nFrom Scratch (Custom CNN):\")\n",
    "print(f\"  Test Accuracy: {test_acc:.4f} ({100*test_acc:.2f}%)\")\n",
    "print(f\"  Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "print(f\"\\nTransfer Learning - Fine-tuning (ResNet18):\")\n",
    "print(f\"  Test Accuracy: {test_acc_ft:.4f} ({100*test_acc_ft:.2f}%)\")\n",
    "print(f\"  Test Loss: {test_loss_ft:.4f}\")\n",
    "print(f\"  Improvement over from-scratch: {100*(test_acc_ft - test_acc):.2f}%\")\n",
    "\n",
    "print(f\"\\nTransfer Learning - Feature Extraction (ResNet18):\")\n",
    "print(f\"  Test Accuracy: {test_acc_conv:.4f} ({100*test_acc_conv:.2f}%)\")\n",
    "print(f\"  Test Loss: {test_loss_conv:.4f}\")\n",
    "print(f\"  Improvement over from-scratch: {100*(test_acc_conv - test_acc):.2f}%\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86ab7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('images/colorful-carpet-sample.png')\n",
    "\n",
    "# transforms for CNN from scratch (must match what the model was trained on, except for batch dimension)\n",
    "small_transform = v2.Compose([  \n",
    "        v2.Resize(size=(64, 64), antialias=True),\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "    ])\n",
    "\n",
    "input_img_small = small_transform(img).unsqueeze(0)\n",
    "input_img_small = input_img_small.to(device)\n",
    "\n",
    "# transforms for transfer learning (must match what the model was trained on, including normalization)\n",
    "transfer_transforms = v2.Compose([\n",
    "    v2.Resize(256),\n",
    "    v2.CenterCrop(224),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                  std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "input_img = transfer_transforms(img).unsqueeze(0)\n",
    "input_img = input_img.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14fa0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all three models on the same image\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON: ALL THREE MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4))\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow(img)\n",
    "axes[0].set_title(\"Original Image\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# From Scratch Model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions_scratch = nn.Softmax(dim=-1)(model(input_img_small)).cpu().numpy()\n",
    "\n",
    "predicted_scratch = np.argmax(predictions_scratch[0])\n",
    "axes[1].bar(train_dataset.dataset.classes, predictions_scratch[0])\n",
    "axes[1].set_title(f\"From Scratch\\nPred: {train_dataset.dataset.classes[predicted_scratch]}\")\n",
    "axes[1].set_ylabel(\"Probability\")\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Fine-tuned Model\n",
    "model_ft.eval()\n",
    "with torch.no_grad():\n",
    "    predictions_ft = nn.Softmax(dim=-1)(model_ft(input_img)).cpu().numpy()\n",
    "\n",
    "predicted_ft = np.argmax(predictions_ft[0])\n",
    "axes[2].bar(transfer_train_dataset.dataset.classes, predictions_ft[0])\n",
    "axes[2].set_title(f\"Fine-tuned (ResNet18)\\nPred: {transfer_train_dataset.dataset.classes[predicted_ft]}\")\n",
    "axes[2].set_ylabel(\"Probability\")\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Feature Extractor Model\n",
    "model_conv.eval()\n",
    "with torch.no_grad():\n",
    "    predictions_conv = nn.Softmax(dim=-1)(model_conv(input_img)).cpu().numpy()\n",
    "\n",
    "predicted_conv = np.argmax(predictions_conv[0])\n",
    "axes[3].bar(transfer_train_dataset.dataset.classes, predictions_conv[0])\n",
    "axes[3].set_title(f\"Feature Extractor (ResNet18)\\nPred: {transfer_train_dataset.dataset.classes[predicted_conv]}\")\n",
    "axes[3].set_ylabel(\"Probability\")\n",
    "axes[3].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFrom Scratch prediction: {train_dataset.dataset.classes[predicted_scratch]} (conf: {predictions_scratch[0][predicted_scratch]:.4f})\")\n",
    "print(f\"Fine-tuned prediction: {transfer_train_dataset.dataset.classes[predicted_ft]} (conf: {predictions_ft[0][predicted_ft]:.4f})\")\n",
    "print(f\"Feature Extractor prediction: {transfer_train_dataset.dataset.classes[predicted_conv]} (conf: {predictions_conv[0][predicted_conv]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc41f88",
   "metadata": {},
   "source": [
    "### Save the Transfer Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f13425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "torch.save(model_ft.state_dict(), model_dir / f\"my_{model_name}_fine_tuned.pt\")\n",
    "print(f\"Fine-tuned model saved to {model_dir / f'my_{model_name}_fine_tuned.pt'}\")\n",
    "\n",
    "# Save the feature extractor model\n",
    "torch.save(model_conv.state_dict(), model_dir / f\"my_{model_name}_feature_extractor.pt\")\n",
    "print(f\"Feature extractor model saved to {model_dir / f'my_{model_name}_feature_extractor.pt'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9ca0af",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### From Scratch vs Transfer Learning:\n",
    "\n",
    "1. **From Scratch (Custom CNN)**:\n",
    "   - Requires more data and training time\n",
    "   - Good for learning fundamentals\n",
    "   - Achieves lower accuracy with limited data\n",
    "\n",
    "2. **Transfer Learning - Fine-Tuning**:\n",
    "   - Leverages pre-trained features (from ImageNet in this case)\n",
    "   - Updates all weights during training\n",
    "   - Usually faster and better accuracy\n",
    "   - Best when you have moderate-sized datasets\n",
    "\n",
    "3. **Transfer Learning - Feature Extraction**:\n",
    "   - Freezes pre-trained layers, trains only final classifier\n",
    "   - Fastest training time\n",
    "   - Good for small datasets\n",
    "   - May not adapt as well to your specific task"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmlap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
