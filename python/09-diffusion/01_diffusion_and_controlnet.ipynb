{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Diffusion with Diffusers\n",
    "\n",
    "This is a notebook that demonstrates how to use the [Diffusers package](https://huggingface.co/docs/diffusers/index) from Huggingface to run stable diffusion. [Huggingface](https://huggingface.co) is a community-driven  platform that provides a comprehensive suite of open-source libraries and tools for machine learning. Think of it as a kind of \"github for machine learning\". The diffusers package provides a simple API and a number of pretrained models that allow to easily run and experiment with diffusion models. \n",
    "\n",
    "For extra references, you can look into the [official HyggingFace inference notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb#scrollTo=yW14FA-tDQ5n) and this [fastAI inference example](https://github.com/fastai/diffusion-nbs/blob/master/stable_diffusion.ipynb)\n",
    "\n",
    "To use this notebook you will need to install diffusers (with your conda environment active) with:\n",
    "```\n",
    "pip install --upgrade diffusers\\[torch\\]\n",
    "pip install transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_NOTE:_**  ONLY IF USING COLAB, run this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade diffusers\\[torch\\]\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then mount your Google Drive so you have access to your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "\n",
    "import os\n",
    "os.chdir('drive/My Drive/DMLAP-2025/python') # change to your directory accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And make sure that you have the \"images\" directory from this week's \"09-diffusion\" copied inside the folder of choice. Alternatively, you may manually upload the dictory to Collab, but it will be gone once you close the notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_NOTE:_**  END COLAB NOTE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Stable Diffusion\n",
    "To run stable diffusion you will need to distinguish wether you are running this on a Mac M1/M2/... or on Linux/Windows with a Nvidia GPU. \n",
    "\n",
    "On a Mac, diffusion will need a \"warmup\" phase to work properly (see [this link](https://huggingface.co/docs/diffusers/optimization/mps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this will load the pretrained model and download it the first time the cell is run (which might take a while). \n",
    "\n",
    "You can set the model version by modifying the `sd_version` variable. Read [this document](https://huggingface.co/docs/diffusers/en/stable_diffusion) for performance recommendations and tricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers import DPMSolverMultistepScheduler\n",
    "\n",
    "sd_version = '2.1'\n",
    "\n",
    "if sd_version == '2.1':\n",
    "    model_key = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "elif sd_version == '2.0':\n",
    "    model_key = \"stabilityai/stable-diffusion-2-base\"\n",
    "elif sd_version == '1.5':\n",
    "    model_key = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_key, \n",
    "    torch_dtype=torch.float16, #loading the model with half-precision floating point numbers, which reduces memory usage and speeds up inference\n",
    "    use_safetensors=True) #avoiding running any malicious code\n",
    "\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config) # faster scheduler for the denoising process\n",
    "\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "if device=='mps':\n",
    "    # Recommended if your computer has < 64 GB of RAM\n",
    "    pipe.enable_attention_slicing()\n",
    "if device=='cuda':\n",
    "    pipe.enable_sequential_cpu_offload()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the image \n",
    "\n",
    "Observe the parameters that we define.\n",
    "\n",
    "Defining a `seed` ensures that the model starts the denoising process from the same initial noise each time, which means you'll get the same output whenever you run the same cell with the same parameters. This is useful for reproducibility. If you skip defining the `generator`, the model will use a random seed each time, which results in different outputs for each run, even with the same prompt.\n",
    "\n",
    "The `guidance_scale` defines the level of influence from the text prompt (positive or negative). Higher values (7-10) mean the model will closely follow the prompt and try to match your description as precisely as possible. Lower values (2-5) give the model more creative freedom and allow for more variation in the generated image, with less strict adherence to the prompt.\n",
    "\n",
    "Increasing `num_inference_steps` will improve quality but will make the denoising process slower. If you don't specify the number of steps, they will be 50 by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.manual_seed(99)\n",
    "\n",
    "prompt = \"A cubist painting of the Star Treck character Spock, high quality, trending on artstation\"\n",
    "image = pipe(\n",
    "    prompt, \n",
    "    guidance_scale=7.5, \n",
    "    generator=generator, \n",
    "    num_inference_steps=20).images[0]\n",
    "\n",
    "plt.imshow(np.array(image))\n",
    "plt.title(prompt)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text guided Image-to-Image Generation\n",
    "\n",
    "We can also use Stable Diffusion with a *seed image*. \n",
    "\n",
    "Load an image of your choice and experiment below.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image \n",
    "\n",
    "seed_image = Image.open(\"images/spock.jpg\")\n",
    "\n",
    "seed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the [`StableDiffusionImg2ImgPipeline`](https://huggingface.co/docs/diffusers/en/api/pipelines/stable_diffusion/img2img) model, similarly to what we did earlier with the normal stable diffusion pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "\n",
    "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "    model_key, \n",
    "    torch_dtype=torch.float16, \n",
    "    use_safetensors=True)\n",
    "\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config) # Faster scheduler\n",
    "\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "if device=='mps':\n",
    "    # Recommended if your computer has < 64 GB of RAM\n",
    "    pipe.enable_attention_slicing()\n",
    "if device=='cuda':\n",
    "    pipe.enable_sequential_cpu_offload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `StableDiffusionImg2ImgPipeline` takes an additional `image` parameter that conditions the diffusion process so that the result is similar to a given image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A punk with a mohawk\"\n",
    "\n",
    "generator = torch.manual_seed(32) \n",
    "\n",
    "image = pipe(prompt, \n",
    "             image=seed_image, # this is the only difference from the previous diffusion example\n",
    "             guidance_scale=7.5, \n",
    "             strength=0.7, \n",
    "             generator=generator, \n",
    "             num_inference_steps=20).images[0]\n",
    "\n",
    "plt.imshow(np.array(image))\n",
    "plt.title(prompt)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diffusers API provides a number of [other pipelines](https://huggingface.co/docs/diffusers/en/api/pipelines/stable_diffusion/overview) that can be useful for other tasks, such as image inpainting or depth-to-image."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditioning Stable Diffusion with ControlNet\n",
    "\n",
    "[ControlNet](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Adding_Conditional_Control_to_Text-to-Image_Diffusion_Models_ICCV_2023_paper.pdf) is a very recent and quite amazing advancement in image generation using stable diffusion. It allows conditioning the stable diffusion generation pipeline on an image input (similarly to pix2pix).  This system operates by integrating a smaller neural network with a pre-trained stable diffusion model. The weights of the stable diffusion model are frozen, and the combined model is trained to steer stable diffusion towards producing images consistent with the provided input conditions.\n",
    "\n",
    "The Huggingface diffusers API comes with ControlNet and a number of pre-trained models that can be used for an number of tasks such as guiding stable diffusion with edges, poses or depth maps (see in the [ControlNet HuggingFace page](https://huggingface.co/lllyasviel/sd-controlnet-canny) for some available models).\n",
    "\n",
    "Here we will use the [\"sd-controlnet-canny\"](https://huggingface.co/lllyasviel/sd-controlnet-canny) model, which guides stable diffusion with edge images. Let's start by using skimage to create edges from an input image, as we did with pix2pix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage import io, feature, transform\n",
    "import PIL.Image as Image\n",
    "import cv2\n",
    "\n",
    "def apply_canny_skimage(img, sigma=1.5, size=512):\n",
    "    import cv2\n",
    "    from skimage import feature\n",
    "    invert = False\n",
    "    grayimg = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    edges = (feature.canny(grayimg, sigma=sigma)*255).astype(np.uint8)\n",
    "    if invert:\n",
    "        edges = cv2.bitwise_not(edges)\n",
    "    return cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "img = io.imread(\"images/spock.jpg\")\n",
    "edges =  apply_canny_skimage(img)\n",
    "\n",
    "# ControlNet expects a PIL Image as input\n",
    "edges_image = Image.fromarray(edges)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.subplot(121)\n",
    "plt.imshow(img)\n",
    "plt.subplot(122)\n",
    "plt.imshow(edges)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we setup the ControlNet model. Note that the current HuggingFace version of ControlNet requires Stable Diffusion 1.5 to run: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n",
    "import PIL.Image as Image\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"lllyasviel/sd-controlnet-canny\", \n",
    "    torch_dtype=torch.float16)\n",
    "    \n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", # Controlnet is currently working only with SD 1.5\n",
    "    torch_dtype=torch.float16,\n",
    "    controlnet=controlnet, \n",
    "    safety_checker=None)\n",
    "\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config) # Faster scheduler\n",
    "\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "if device=='mps':\n",
    "    pipe.enable_attention_slicing()\n",
    "if device=='cuda':\n",
    "    pipe.enable_sequential_cpu_offload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And generate an image, by passing the edge image as an input to the `image` argument. We will use 15 inference steps here to save a little bit of time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A sculpture made of clay on a white background\"\n",
    "\n",
    "generator = torch.manual_seed(0) # Removing manual_seed will always generate different images \n",
    "\n",
    "image = pipe(\n",
    "    prompt,\n",
    "    image=edges_image,\n",
    "    generator=generator,\n",
    "    num_inference_steps=15,\n",
    ").images[0]\n",
    "\n",
    "plt.imshow(np.array(image))\n",
    "plt.title(prompt)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedural Input for Image Generation\n",
    "\n",
    "In the previous example, we used edge detection to produce the ControlNet condition image. However, you can create interesting visual results also by procedurally generating an *edge-like* image. We will use the [py5canvas](https://github.com/colormotor/py5canvas) API by Daniel Berio, to make things easier (as long as you are able to install it correctly). Here are some updated instructions that should work, in case you have not installed it yet. **Skip these steps if you have already installed and used py5canvas**. \n",
    "\n",
    "With you environment active:\n",
    "```\n",
    "pip install face_recognition\n",
    "pip install pyglet\n",
    "```\n",
    "Then:\n",
    "```\n",
    "conda install conda-forge::cairo\n",
    "```\n",
    "Followed by:\n",
    "```\n",
    "conda install conda-forge::pycairo\n",
    "```\n",
    "Finally install py5canvas with: \n",
    "```\n",
    "pip install git+https://github.com/colormotor/py5canvas.git\n",
    "```\n",
    "\n",
    "Now we can crete some graphics:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_NOTE:_**  ONLY IF USING COLAB, run this first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt install libcairo2-dev pkg-config python3-dev\n",
    "!pip install git+https://github.com/colormotor/py5canvas.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_NOTE:_**  END COLAB NOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py5canvas import canvas\n",
    "c = canvas.Canvas(512, 512)\n",
    "\n",
    "np.random.seed(123)\n",
    "c.background(0)\n",
    "c.stroke_weight(2)\n",
    "c.stroke(255)\n",
    "c.no_fill()\n",
    "#c.set_rect_mode('center')\n",
    "for i in range(40):\n",
    "    c.rectangle(np.random.uniform(0, c.width, 2), np.random.uniform(10, 90, 2))\n",
    "\n",
    "# Retrieve the canvas image and convert it from a numpy array to a PIL Image. \n",
    "img = c.get_image()\n",
    "edges_image = Image.fromarray(img)\n",
    "# show it\n",
    "edges_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this case, we may get better results by using the [\"sd-controlnet-scribble\"](https://huggingface.co/lllyasviel/sd-controlnet-scribble) model, which is similar to the Canny edge detection model, but is trained on scribble-like images instead of edge maps. Note that both the *scribble* and *canny* control net are trained on images with that have a specific line width and the stroke width of the generated condition image will affect the quality of the results. Try to adjust the line width using the `stroke_weight` function and tweak it to adjust the generated results to your preference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"lllyasviel/sd-controlnet-scribble\", \n",
    "    torch_dtype=torch.float16)\n",
    "    \n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", # Controlnet is currently working only with SD 1.5\n",
    "    torch_dtype=torch.float16,\n",
    "    controlnet=controlnet, \n",
    "    safety_checker=None)\n",
    "\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config) # Faster scheduler\n",
    "\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "if device=='mps':\n",
    "    pipe.enable_attention_slicing()\n",
    "if device=='cuda':\n",
    "    pipe.enable_sequential_cpu_offload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we will also add a `negative_prompt`, which is a caption describing outputs we wish to avoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A white wall with many portraits of Star Trek character Spock\"\n",
    "\n",
    "generator = torch.manual_seed(420) # Removing the generator will always result in different images \n",
    "\n",
    "image = pipe(\n",
    "    prompt,\n",
    "    negative_prompt=\"blurry, user interface, captions\",\n",
    "    image=edges_image,\n",
    "    generator=generator,\n",
    "    controlnet_conditioning_scale=0.9,\n",
    "    guidance_scale=7.5, \n",
    "    guess_mode=False, # Guess mode will try to \"guess\" the contents of the input, so it can be used also without a prompt\n",
    "    num_inference_steps=10,\n",
    ").images[0]\n",
    "\n",
    "plt.imshow(np.array(image))\n",
    "plt.title(prompt)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedural Input for Animations\n",
    "\n",
    "We can also create a procedural animation using a similar technique together with ffmpeg. \n",
    "\n",
    "Let's create an animation of waves, saving each frame of the animation in a *input_animation_frames* directory (for later visualisation) and storing the frames in a `frame_images` list. For time/performance consideration, we will create only 10 frames for the moment. This will result in a bit of a choppy animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from py5canvas import canvas\n",
    "\n",
    "# Create a folder for input animation \n",
    "output_folder = 'input_animation_frames'\n",
    "if not os.path.isdir(output_folder):\n",
    "    os.mkdir(output_folder)\n",
    "\n",
    "# Modify this function for different animations\n",
    "def animation_frame(c, frame_index, n_frames):\n",
    "    c.background(0)\n",
    "    c.stroke_weight(1) \n",
    "    c.stroke(255)\n",
    "    c.no_fill()\n",
    "\n",
    "    phase = (np.pi*2)/n_frames * frame_index\n",
    "    for y in np.linspace(0, 1, 13)[1:-1]: # Loop 0-1 skipping first and last row (\n",
    "        c.begin_shape()\n",
    "        for x in np.linspace(0, 1, 100):\n",
    "            c.vertex(x*c.width, y*c.height +\n",
    "                     np.sin(x*np.pi*4 + phase + y*6)*np.cos(y*np.pi*6 + x*5.2)*20) # Bit arbitrary moving waves creating a loop\n",
    "        c.end_shape()\n",
    "    c.save_image(os.path.join(output_folder, 'frame_%02d.png'%(frame_index+1)))\n",
    "    return Image.fromarray(c.get_image())\n",
    "\n",
    "# Create the canvas once\n",
    "c = canvas.Canvas(512, 512)\n",
    "\n",
    "# comment this block and uncomment the last line to preview one frame\n",
    "n_frames = 10\n",
    "frame_images = []\n",
    "for i in range(n_frames):\n",
    "    frame_images.append( animation_frame(c, i, n_frames) )\n",
    "# preview one frame\n",
    "frame_images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above previews one frame of the animation. Now let's use ffmpeg to convert the saved frames to an animated gif that we can visualise in the notebook. We will use a very low frame rate (10) so the animation with 10 frames will last one second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -y -f image2 -framerate 10 -i 'input_animation_frames/frame_%02d.png' -loop 0 input_animation.gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command above (as other command-line commands can be) is quite cryptic to read. Look into ffmpeg documentation if in need for clarifications.\n",
    "\n",
    "Now to visualise the animation we can use the `IPython` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.display.Image('input_animation.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's take the image for each frame and use ControlNet to to transform it. \n",
    "\n",
    "Note that we force the seed to be the same for each iteration of the loop. This will give some form of temporal coherence to the animation. Nevertheless, the animation will probably flicker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder for input animation \n",
    "output_folder = 'output_animation_frames'\n",
    "if not os.path.isdir(output_folder):\n",
    "    os.mkdir(output_folder)\n",
    "\n",
    "prompt = \"Colorful 3d ribbons, Cinema 4d, Maxon renderer\"\n",
    "preview = -1 # Set to 0 or greater to just preview a frame, -1 for saving video\n",
    "generated_images = []\n",
    "for i, frame in enumerate(frame_images):\n",
    "    print('Frame %d of %d'%(i+1, len(frame_images)))\n",
    "    generator = torch.manual_seed(677) # Removing the generator will always result in different images \n",
    "    if preview > -1:\n",
    "        frame = frame_images[preview]\n",
    "\n",
    "    image = pipe(\n",
    "        prompt,\n",
    "        negative_prompt=\"blurry, user interface, captions\",\n",
    "        image=frame,\n",
    "        generator=generator,\n",
    "        controlnet_conditioning_scale=0.7,\n",
    "        guidance_scale=6.5, \n",
    "        guess_mode=False, # Guess mode will try to \"guess\" the contents of the input, so it can be used also without a prompt\n",
    "        num_inference_steps=10, # 10 inference steps is faster and seems to be enough\n",
    "    ).images[0]\n",
    "    if preview > -1:\n",
    "        display(image)\n",
    "        break\n",
    "    generated_images.append(image)\n",
    "    image.save(os.path.join(output_folder, 'frame_%02d.png'%(i+1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again let's create a gif, this time from the *output_animation_frames* directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -y -f image2 -framerate 10 -i 'output_animation_frames/frame_%02d.png' -loop 0 output_animation.gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And visualise it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Image('output_animation.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Things to Experiment with:\n",
    "\n",
    "- Run the full notebook and understand how the different approaches work\n",
    "\n",
    "- Test out the notebook with your own prompts, search for prompting tips and tricks, play around with negative prompting, etc\n",
    "\n",
    "- Test out the notebook with different parameters to direct the generated outcome to your likings\n",
    "\n",
    "- Introduce computational thinking to your making process, eg build your prompt generator and use it with SD, export generated images in each inference step and use these as seed_images for the next generation, etc\n",
    "\n",
    "- Search for different models from HuggingFace and use them in the same way. Eg. for upscaling images, look into [this notebook](https://github.com/huggingface/notebooks/blob/main/diffusers/latent_diffusion_upscaler.ipynb). The model was originally released in [Latent Diffusion repo](https://github.com/CompVis/latent-diffusion). It's a simple, 4x super-resolution model diffusion model. This model is not conditioned on text.\n",
    "\n",
    "    `from diffusers import LDMSuperResolutionPipeline`\n",
    "\n",
    "    `pipe = LDMSuperResolutionPipeline.from_pretrained(\"CompVis/ldm-super-resolution-4x-openimages\").to(device)`\n",
    "\n",
    "    `...`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmlap25my",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
